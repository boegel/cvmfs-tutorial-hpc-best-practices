{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Best Practices for CernVM-FS in HPC","text":"<p>Warning</p> <p>(Nov'23) This tutorial is under development, please come back later when the tutorial contents have been completed.</p> <p>An online version of this tutorial is planned for Mon 4 Dec 2023 (13:30-17:00 CET), register via https://event.ugent.be/registration/cvmfshpc202312.</p> <p>This is an introductory tutorial to CernVM-FS, the CernVM File System, with a focus on employing it in the context of High-Performance Computing (HPC).</p> <p>In this tutorial you will learn what CernVM-FS is, how to get access to existing CernVM-FS repositories, how to configure CernVM-FS, and how to use CernVM-FS repositories on HPC infrastructure.</p> <p>Ready to go? Click here to start the tutorial!</p>"},{"location":"#intended-audience","title":"Intended audience","text":"<p>This tutorial is intended for people with a background in HPC (system administrators, support team members, end users, etc.) and who are new to CernVM-FS; no specific prior knowledge or experience with it is required.</p> <p>We expect it to be most valuable to people who are interested in using or providing access to one or more existing CernVM-FS repositories on HPC infrastructure.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>(more info soon)</p>"},{"location":"#practical-information","title":"Practical information","text":"<p>A first virtual edition of this tutorial is planned for Monday 4 December 2023 (13:30-17:00 CET).</p> <p>Attendance is free, but registration is required: https://event.ugent.be/registration/cvmfshpc202312.</p> <p>(more practical info soon)</p>"},{"location":"#slides","title":"Slides","text":"<p>(coming soon)</p>"},{"location":"#multixscale","title":"MultiXscale","text":"<p>This tutorial is being developed and organised in the context of the MultiXscale EuroHPC Centre-of-Excellence.</p> <p>Funded by the European Union. This work has received funding from the European High Performance Computing Joint Undertaking (JU) and countries participating in the project under grant agreement No 101093169.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Jakob Blomer (CERN, Switzerland)</li> <li>Bob Dr\u00f6ge (University of Groningen, The Netherlands)</li> <li>Kenneth Hoste (Ghent University, Belgium)</li> <li>Alan O'Cais (University of Barcelona, Spain; CECAM)</li> <li>Lara Peeters (Ghent University, Belgium)</li> <li>Laura Promberger (CERN, Switzerland)</li> <li>Thomas R\u00f6blitz (University of Bergen, Norway)</li> <li>Caspar van Leeuwen (SURF, The Netherlands)</li> <li>Valentin V\u00f6lkl (CERN, Switzerland)</li> </ul>"},{"location":"#additional-resources","title":"Additional resources","text":"<ul> <li>CernVM-FS website</li> <li>CernVM-FS documentation</li> <li>CernVM-FS @ GitHub</li> <li>Introduction to CernVM-FS by Jakob Blomer (CERN) (2021)</li> <li>Introductory tutorial on CernVM-FS (2021)</li> </ul>"},{"location":"configuration_hpc/","title":"Configuring CernVM-FS on HPC infrastructure","text":""},{"location":"configuration_hpc/#diskless-workernodes","title":"Diskless workernodes","text":""},{"location":"configuration_hpc/#offline-workernodes","title":"Offline workernodes","text":""},{"location":"configuration_hpc/#alien-cache","title":"Alien cache","text":""},{"location":"configuration_hpc/#security","title":"Security","text":""},{"location":"configuration_hpc/#syncing-a-cernvm-fs-repository-to-another-filesystem","title":"Syncing a CernVM-FS repository to another filesystem","text":""},{"location":"containers/","title":"Containers and CernVM-FS","text":""},{"location":"containers/#accessing-a-cernvm-fs-repository-via-apptainer","title":"Accessing a CernVM-FS repository via Apptainer","text":""},{"location":"containers/#ingesting-container-images-in-a-cernvm-fs-repository","title":"Ingesting container images in a CernVM-FS repository","text":""},{"location":"getting-started/","title":"Getting started with CernVM-FS (from scratch)","text":""},{"location":"getting-started/#setting-up-the-stratum-0-server","title":"Setting up the Stratum-0 server","text":""},{"location":"getting-started/#creating-a-cernvm-fs-repository","title":"Creating a CernVM-FS repository","text":""},{"location":"getting-started/#setting-up-a-stratum-1-replica-server","title":"Setting up a Stratum-1 replica server","text":""},{"location":"performance/","title":"Performance aspects of CernVM-FS","text":""},{"location":"performance/#startup-performance","title":"Startup performance","text":""},{"location":"performance/#os-jitter-by-cernvm-fs-daemon","title":"OS jitter by CernVM-FS daemon","text":""},{"location":"performance/#using-a-cdn","title":"Using a CDN","text":""},{"location":"storage-backends/","title":"Different storage backends for CernVM-FS","text":""},{"location":"storage-backends/#s3","title":"S3","text":""},{"location":"storage-backends/#tradeoffs","title":"Tradeoffs","text":""},{"location":"troubleshooting/","title":"Troubleshooting and debugging CernVM-FS","text":""},{"location":"troubleshooting/#logs","title":"Logs","text":""},{"location":"troubleshooting/#stats","title":"Stats","text":""},{"location":"troubleshooting/#common-problems","title":"Common problems","text":""},{"location":"troubleshooting/#monitoring","title":"Monitoring","text":""},{"location":"troubleshooting/#mounting-in-debug-mode","title":"Mounting in debug mode","text":""},{"location":"access/","title":"Accessing CernVM-FS repositories","text":"<ul> <li>Setting up a CernVM-FS client system</li> <li>Setting up a proxy server</li> <li>Setting up a Stratum 1 replica server</li> <li>Alternative ways to access CernVM-FS repositories</li> </ul>"},{"location":"access/alternatives/","title":"Alternative ways to access CernVM-FS repositories","text":"<p>(next: Configuration on HPC systems)</p>"},{"location":"access/client/","title":"Setting up a CernVM-FS client system","text":"<p>The recommended way to gain access to CernVM-FS repositories is to set up a system-wide native installation of CernVM-FS on the client system(s), which comes down to:</p> <ul> <li>Installing the client component of CernVM-FS;</li> <li>Creating a minimal client configuration file (<code>/etc/cvmfs/default.local</code>);</li> <li>Completing the client setup by:<ul> <li>Creating a<code>cvmfs</code> user account and group;</li> <li>Creating the <code>/cvmfs</code>, <code>/var/lib/cvmfs</code> directories;</li> <li>Configuring <code>autofs</code> to enable auto-mounting of CernVM-FS repositories (optional).</li> </ul> </li> </ul> <p>For repositories that are not included in the default CernVM-FS configuration you also need to provide some additional information specific to those repositories in order to access them.</p> <p>This is not a production-ready setup (yet)!</p> <p>While these basic steps are enough to gain access to CernVM-FS repositories, this is not sufficient to obtain a production-ready setup.</p> <p>This is especially true on HPC infrastructure that typically consists of a large number of worker nodes on which software provided by one or more CernVM-FS repositories will be used.</p> <p>After covering the basic steps in this section, we will outline how to make the setup more reliable and performant, by setting up a proxy server and CernVM-FS Stratum 1 replica server.</p>"},{"location":"access/client/#installing-cernvm-fs-client","title":"Installing CernVM-FS client","text":"<p>Start with installing the <code>cvmfs</code> package which provides the CernVM-FS client component:</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code># install cvmfs-release package to add yum repository\nsudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\n\n# install CernVM-FS client package\nsudo yum install -y cvmfs\n</code></pre> <pre><code># install cvmfs-release package to add apt repository\nsudo apt install lsb-release\ncurl -OL https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest_all.deb\nsudo dpkg -i cvmfs-release-latest_all.deb\nsudo apt update\n\n# install CernVM-FS client package\nsudo apt install -y cvmfs\n</code></pre> <p>If none of the available <code>cvmfs</code> packages are compatible with your system, you can also build CernVM-FS from source.</p>"},{"location":"access/client/#minimal_configuration","title":"Minimal client configuration","text":"<p>Next to installing the CernVM-FS client, you should also create a minimal configuration file for it.</p> <p>This is typically done in <code>/etc/cvmfs/default.local</code>, which should contain something like:</p> <pre><code>CVMFS_CLIENT_PROFILE=\"single\"\nCVMFS_QUOTA_LIMIT=10000\n</code></pre> <p>More information on the structure of <code>/etc/cvmfs</code> and supported configuration settings is available in the CernVM-FS documentation.</p>"},{"location":"access/client/#client-profile-setting","title":"Client profile setting","text":"<p>With <code>CVMFS_CLIENT_PROFILE=\"single\"</code> we specify that this CernVM-FS client should:</p> <ul> <li>Use the proxy server specified via <code>CVMFS_HTTP_PROXY</code>, if that configuration setting is defined;</li> <li>Directly connect to a Stratum-1 replica server that provides the repository being used if no proxy server   is specified via <code>CVMFS_HTTP_PROXY</code>.</li> </ul> <p>As an alternative to defining <code>CVMFS_CLIENT_PROFILE</code>, you can also set <code>CVMFS_HTTP_PROXY</code> to <code>DIRECT</code> to specify that no proxy server should be used by CernVM-FS:</p> <pre><code>CVMFS_HTTP_PROXY=\"DIRECT\"\n</code></pre> <p>We will get back to <code>CVMFS_HTTP_PROXY</code> later when setting up a proxy server.</p>"},{"location":"access/client/#quota-limit-for-client-cache","title":"Quota limit for client cache","text":"<p>The <code>CVMFS_QUOTA_LIMIT</code> configuration setting specifies the maximum size of the CernVM-FS client cache (in MBs).</p> <p>In the example above, we specify that no more than ~10GB should be used for the client cache.</p> <p>Once the quota limit is reached, CernVM-FS will automatically remove files from the cache according to the Least Recently Used (LRU) policy, until half of the maximum cache size has been freed.</p> <p>The location of the cache directory can be controlled by <code>CVMFS_CACHE_BASE</code> if needed (default: <code>/var/run/cvmfs</code>), but must be a on a local file system of the client, not a network file system that can be modified by multiple hosts.</p> <p>For more information on cache-related configuration settings, see the CernVM-FS documentation.</p>"},{"location":"access/client/#completing-the-client-setup","title":"Completing the client setup","text":"<p>To complete the setup of the CernVM-FS client component, a<code>cvmfs</code> user account and group must be added to the system, and the <code>/cvmfs</code> and <code>/var/lib/cvmfs</code> directories must be created.</p> <p>In addition, it is recommended to update the <code>autofs</code> configuration to enable auto-mounting of CernVM-FS repositories (optional).</p> <p>All these actions can be performed in one go by running the following command:</p> <pre><code>sudo cvmfs_config setup\n</code></pre> <p>Additional options can be passed to this command to disable some of the actions, like <code>nouser</code> to not create the <code>cvmfs</code> user and group, or <code>noautofs</code> not update the <code>autofs</code> configuration.</p> Impact of not updating <code>autofs</code> configuration (click to expand) <p>If you prefer not to use <code>autofs</code>, you will either need to:</p> <ul> <li> <p>Manually mount the CernVM-FS repositories you want to use, for example:   <pre><code>sudo mount -t cvmfs software.eessi.io /cvmfs/software.eessi.io\n</code></pre></p> </li> <li> <p>or update <code>/etc/fstab</code> to ensure that the CernVM-FS repositories   are mounted at boot time.</p> </li> </ul> <p>For more information on mounting repositories, see the CernVM-FS documentation.</p>"},{"location":"access/client/#checking-client-setup","title":"Checking client setup","text":"<p>To ensure that the setup of the CernVM-FS client component is valid, you can run:</p> <pre><code>sudo cvmfs_config chksetup\n</code></pre> <p>You should see <code>OK</code> as output of this command.</p>"},{"location":"access/client/#default-repositories","title":"Default repositories","text":"<p>The default configuration of CernVM-FS, provided by the <code>cvmfs-config-default</code> packages, provides the public keys and configuration for a number of commonly used CernVM-FS repositories.</p> <p>One particular repository included in the default CernVM-FS configuration is <code>cvmfs-config.cern.ch</code>, which is a CernVM-FS config repository that provides public keys and configuration for additional flagship CernVM-FS repositories, like EESSI.</p> <p>To check whether a specific repository is accessible, we can probe it:</p> <pre><code>$ cvmfs_config probe software.eessi.io\nProbing /cvmfs/software.eessi.io... OK\n</code></pre> <p>To view the configuration for a specific repository, use <code>showconfig</code>: <pre><code>cvmfs_config showconfig software.eessi.io\n</code></pre></p>"},{"location":"access/client/#additional-repositories","title":"Additional repositories","text":"<p>To access additional CernVM-FS repositories beyond those that are available by default, you will need to:</p> <ul> <li>Add the public keys for those repositories into a domain-specific subdirectory of <code>/etc/cvmfs/keys/</code>;</li> <li>Add the configuration for those repositories into <code>/etc/cvmfs/domain.d</code> (domain-specific) or <code>/etc/cvmfs/config.d</code> (repository-specific).</li> </ul> <p>For examples, see the <code>/etc/cvmfs</code> subdirectory in the config-repo GitHub repository.</p> <p>(next: Setting up a proxy server)</p>"},{"location":"access/proxy/","title":"Setting up a proxy server","text":"<p>As a first step towards a production-ready CernVM-FS setup we will install a Squid forward proxy server, which is strongly recommended in the context of HPC systems.</p> <p>The proxy server will (often dramatically) reduce the latency for client systems, and hence significantly improve start-up performance of software provided via a CernVM-FS repository. In addition, it reduces the load on the Stratum 1 replica servers that support the CernVM-FS repositories being used.</p> <p>This is particularly important when running large-scale MPI software, since starting the software requires that the corresponding binary and the libraries it depends on are available on all worker nodes being employed.</p>"},{"location":"access/proxy/#general-recommendations","title":"General recommendations","text":"<p>It is strongly recommended to have at least two proxy servers available, to have some redundancy available in case of unexpected problems or when performance maintenance.</p> <p>As a rule of thumb, it is recommended to have (at least) one proxy server for every couple of hundred worker nodes (100-500).</p> <p>The load on the proxy servers is highly dependent on the workload mix on the client systems.</p>"},{"location":"access/proxy/#proxy-server-setup","title":"Proxy server setup","text":""},{"location":"access/proxy/#installation","title":"Installation","text":"<p>First, install the <code>squid</code> package using your OS package manager:</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code>sudo yum install -y squid\n</code></pre> <pre><code>sudo apt install -y squid\n</code></pre>"},{"location":"access/proxy/#configuration","title":"Configuration","text":"<p>Create a configuration file for the Squid proxy in <code>/etc/squid/squid.conf</code>.</p> <p>You can use the following template for this:</p> <pre><code># List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy\nacl local_nodes src YOUR_CLIENT_IPS\n\n# Destination domains that are allowed\n#acl stratum_ones dstdomain .YOURDOMAIN.ORG\n#acl stratum_ones dstdom_regex YOUR_REGEX\n\n# Squid port\nhttp_port 3128\n\n# Deny access to anything which is not part of our stratum_ones ACL.\nhttp_access deny !stratum_ones\n\n# Only allow access from our local machines\nhttp_access allow local_nodes\nhttp_access allow localhost\n\n# Finally, deny all other access to this proxy\nhttp_access deny all\n\nminimum_expiry_time 0\nmaximum_object_size 1024 MB\n\n# proxy memory cache of 1GB\ncache_mem 1024 MB\nmaximum_object_size_in_memory 128 KB\n# 50 GB disk cache\ncache_dir ufs /var/spool/squid 50000 16 256\n</code></pre> <p>In this template, you must change two things in the Access Control List (ACL) settings:</p> <p>1) Specify which client systems can access your proxy by replacing \"<code>YOUR_CLIENT_IPS</code>\" with the corresponding IP range, using CIDR notation;</p> <p>2) Make sure that Squid allows access to all Stratum 1 replica servers that are relevant for the CernVM-FS repositories    you are using, by specifying an ACL for each of them via a line that starts with \"<code>acl stratum_ones</code>\"    (see also the Squid ACL documentation).</p> <p>For example, to allow the EESSI Stratum 1 replica servers, you can use:</p> <pre><code>acl stratum_ones dstdomain .eessi.science\n</code></pre> <p>To check your Squid configuration, use:</p> <pre><code>squid -k parse\n</code></pre> <p>If no warnings or errors are printed by this command, you should be all set (provided that the ACLs are set correctly).</p> <p>For more information on configuring a Squid proxy, see the CernVM-FS documentation,</p>"},{"location":"access/proxy/#starting-the-service","title":"Starting the service","text":"<p>To start the Squid and enable it on booting the proxy server, run:</p> <pre><code>sudo systemctl start squid\nsudo systemctl enable squid\n</code></pre> <p>To check the status of the Squid, you can use:</p> <pre><code>sudo systemctl status squid\n</code></pre>"},{"location":"access/proxy/#client-system-configuration","title":"Client system configuration","text":"<p>To make a CernVM-FS client system use the proxy server, the <code>/etc/cvmfs/default.local</code> configuration file on the client system should be updated to include:</p> <pre><code>CVMFS_HTTP_PROXY=\"http://&lt;PROXY_IP&gt;:3128\"\n</code></pre> <p>in which \"<code>&lt;PROXY_IP&gt;</code>\" is replaced with the IP address or hostname of the proxy server.</p> <p>To apply the change we need to reload the CernVM-FS configuration:</p> <pre><code>sudo cvmfs_config reload\n</code></pre> <p>You can test the new configuration and verify whether the proxy is indeed being used via <code>cvmfs_config stat</code>:</p> <pre><code>ls /cvmfs/software.eessi.io\ncvmfs_config stat -v software.eessi.io\n</code></pre> <p>Here we fist inspect the contents of the repository using <code>ls</code> to make sure that the repository is mounted, which is assumed by <code>cvmfs_config stat</code>.</p> <p>The output of the <code>stat</code> command should include a link like this:</p> <pre><code>Connection: .../software.eessi.io through proxy http://&lt;PROXY_IP&gt;:3128 (online)\n</code></pre> <p>(next: Setting up a Stratum 1 replica server)</p>"},{"location":"access/stratum1/","title":"Setting up a Stratum 1 replica server","text":"<p>(next: Alternative ways to access CernVM-FS repositories)</p>"},{"location":"appendix/terminology/","title":"CernVM-FS Terminology","text":"<p>An overview of terms used in the context of CernVM-FS, in alphabetical order.</p>"},{"location":"appendix/terminology/#catalog","title":"Catalog","text":"<p>A catalog of a CernVM-FS repository is a table that lists files and directories along with the corresponding metadata (sizes, timestamps, etc.).</p> <p>Catalogs can be nested: subtrees of the repository may have their own catalog.</p> <p>For more information on the catalog concept, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#cernvm","title":"CernVM","text":"<p>CernVM is a virtual machine image based on CentOS combined with a custom, virtualization-friendly Linux kernel, and which includes the CernVM-FS client software.</p> <p>It is used for the CERN Large Hadron Collider (LHC) experiment, and was created to remove a need for the installation of the experiment software and to minimize the number of platforms (compiler-OS combinations) on which experiment software needs to be supported and tested.</p> <p>While originally developed in conjunction, the CernVM File System today is a product that is completely independent from the CernVM virtual appliance.</p> <p>For more information on CernVM, see the website and documentation.</p>"},{"location":"appendix/terminology/#cvmfs","title":"CernVM-FS","text":"<p>(see What is CernVM-FS?)</p>"},{"location":"appendix/terminology/#client","title":"Client","text":"<p>A client in the context of CernVM-FS is a computer system on which a CernVM-FS repository is being accessed, on which it will be presented as a POSIX read-only file system in a subdirectory of <code>/cvmfs</code>.</p>"},{"location":"appendix/terminology/#proxy","title":"Proxy","text":"<p>A proxy, also referred to as squid proxy, is a forward caching proxy server which acts as an intermediary between a CernVM-FS client and the Stratum-1 replica servers.</p> <p>It is used to improve the latency observed when accessing the contents of a repository, and to reduce the load on the Stratum-1 replica servers.</p> <p>A commonly used proxy is Squid.</p> <p>For more information on proxies, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#publishing","title":"Publishing","text":"<p>Publishing is the process of adding more files to a CernVM-FS repository, which is done via a transaction mechanism, and is on possible on the Stratum-0 server, via a publisher, or via a repository gateway.</p> <p>The workflow of publishing content is covered in detail in the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#repository","title":"Repository","text":"<p>A CernVM-FS repository is where the files and directories that you want to distribute via CernVM-FS are stored, which usually correspond to a collection of software installations.</p> <p>It is a form of content-addressable storage (CAS), and is the single source of (new) data for the file system being presented as a subdirectory of <code>/cvmfs</code> on client systems that mount the repository.</p> <p>Note</p> <p>A CernVM-FS repository includes software installations, not software packages like RPMs.</p>"},{"location":"appendix/terminology/#software-installations","title":"Software installations","text":"<p>An important distinction for a CernVM-FS repository compared to the more traditional notion of a software repository is that a CernVM-FS repository provides access to the individual files that collectively form a particular software installation, as opposed to housing a set of software packages like RPMs, each of which being a collection of files for a particular software installation that are packed together in a single package to distribute as a whole.</p> <p>Note</p> <p>This is an important distinction, since CernVM-FS enables only downloading the specific files that are required to perform a particular task with a software installation, which often is a small subset of all files that are part of that software installation.</p>"},{"location":"appendix/terminology/#stratum0","title":"Stratum 0 server","text":"<p>A Stratum 0 server, often simply referred to a Stratum 0 (Stratum Zero), is the central server for one or more CernVM-FS repositories.</p> <p>It is the single source of (new) data, since it hosts the master copy of the repository contents.</p> <p>Adding or updating files in a CernVM-FS repository (publishing) can only be done on the Stratum 0 server, either directly via the <code>cvmfs_server publish</code> command, or indirectory via a publisher server.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#stratum1","title":"Stratum 1 replica server","text":"<p>A Stratum 1 replica server, often simply referred to a Stratum 1 (Stratum One), is a standard web server that acts as a mirror server for one or more CernVM-FS repositories.</p> <p>It holds a complete copy of the data for each CernVM-FS repository it serves, and automatically synchronises with the Stratum 0.</p> <p>There is typically a network of several Stratum 1 servers for a CernVM-FS repository, which are geographically distributed.</p> <p>Clients can be configured to automatically connect to the closest Stratum 1 server by using the CernVM-FS GeoAPI.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"cvmfs/","title":"Introduction to CernVM-FS","text":"<ul> <li>What is CernVM-FS?</li> <li>Technical details</li> <li>Flagship repositories</li> </ul>"},{"location":"cvmfs/flagship-repositories/","title":"Flagship CernVM-FS repositories","text":""},{"location":"cvmfs/flagship-repositories/#lhc-experiments","title":"LHC experiments","text":"<p>CernVM-FS repositories are used to distribute the software required to analyse the data produced by the Large Hadron Collider (LHC) at each of the LHC experiments.</p> <p>Examples include (click to browse repository contents):</p> <ul> <li><code>/cvmfs/alice.cern.ch</code>: software for ALICE experiment</li> <li><code>/cvmfs/atlas.cern.ch</code>: software for ATLAS experiment</li> <li><code>/cvmfs/cms.cern.ch</code>: software for CMS experiment</li> <li><code>/cvmfs/lhcb.cern.ch</code>: software for LHCb experiment</li> <li><code>/cvmfs/sft.cern.ch</code>: LCG Software Stacks</li> </ul>"},{"location":"cvmfs/flagship-repositories/#the-alliance","title":"The Alliance","text":"<p>The Digital Research Alliance of Canada, a.k.a. The Alliance and formerly known as Compute Canada, uses CernVM-FS to distribute the software stack for the Canadian national compute clusters.</p> <p>Documentation on using their CernVM-FS repository <code>/cvmfs/soft.computecanada.ca</code> is available here, and an overview of available software is available here.</p>"},{"location":"cvmfs/flagship-repositories/#unpacked-containers","title":"Unpacked containers","text":"<p>CernVM-FS repositories can be used to provide an efficient way to access container images, by serving unpacked container images that can be consumed by container runtimes such as Apptainer.</p> <p>Examples include:</p> <ul> <li><code>/cvmfs/unpacked.cern.ch</code></li> <li><code>/cvmfs/singularity.opensciencegrid.org</code></li> </ul> <p>More information on <code>unpacked.cern.ch</code> is available in the CernVM-FS documentation:</p> <ul> <li>Container Images and CernVM-FS</li> <li>Working with DUCC and Docker Images</li> </ul>"},{"location":"cvmfs/flagship-repositories/#eessi","title":"EESSI","text":"<p>The European Environment for Scientific Software Installations (EESSI) provides optimized installations of scientific software for <code>x86_64</code> (Intel + AMD) and <code>aarch64</code> (64-bit Arm) systems that work on any Linux distribution.</p> <p>We will use EESSI as an example CernVM-FS repository throughout this tutorial.</p> <p>(next: What is EESSI?)</p>"},{"location":"cvmfs/technical-details/","title":"Technical details of CernVM-FS","text":"<p>CernVM-FS is implemented as a POSIX read-only filesystem in user space (FUSE) with repositories of files that are served via outgoing HTTP connections only, thus avoiding problems with firewalls.</p> <p>Files in a CernVM-FS repository are automatically downloaded on-demand to a client system as they are accessed, from web servers that support the CernVM-FS repository being used.</p> <p>Internally, CernVM-FS uses content-adressable storage (CAS) and Merkle trees (like Git also does) to store file data and metadata.</p>"},{"location":"cvmfs/technical-details/#caching","title":"Caching","text":"<p>CernVM-FS uses a caching mechanism with a least-recently used (LRU) cache replacement policy, in which configurable local client cache is populated via either a forward proxy server (like Squid), or from a Stratum-1 replica server.</p> <p>Both the proxy and the replica server could be within the same local network as the client, or not.</p> <p>To help reduce performance problems regarding network latency and bandwidth, clients can leverage the Geo API supported by CernVM-FS Stratum-1 replica servers to automatically sort them geographically, in order to prioritize connecting to the closest ones.</p> <p>Furthermore, additional caches can be made available to CernVM-FS, such as an alien cache on a shared cluster filesystem like GPFS or Lustre that is not managed by CernVM-FS, and a Content Delivery Network (CDN) can be used to help limit the time required to download files that are not cached yet.</p> <p>(next: Flagship CernVM-FS repositories)</p>"},{"location":"cvmfs/what-is-cvmfs/","title":"What is CernVM-FS?","text":"<p>CernVM-FS, the CernVM File System (also known as CVMFS), is a file distribution service that is particularly well suited to distribute software installations across a large number of systems world-wide in an efficient way.</p> <p>From an end user perspective, files in a CernVM-FS repository are available read-only via a subdirectory in <code>/cvmfs</code>, with a user experience similar to that of an on-demand streaming service for music or video, but then (mainly) applied to software installations.</p>"},{"location":"cvmfs/what-is-cvmfs/#primary-use-case","title":"Primary use case","text":"<p>The primary use case of CernVM-FS is distributing software, and it provides several interesting features that support this, including:</p> <ul> <li>on-demand downloading and updating of repository contents;</li> <li>multi-level caching;</li> <li>de-duplication of files;</li> <li>compression of data;</li> <li>verification of data integrity;</li> </ul> <p>CernVM-FS has been proven to scale to billions of files and tens of thousands of clients.</p> <p>It was originally developed at CERN to let High Energy Physics (HEP) collaborations like the experiments at the Large Hadron Collider (LHC) deploy software on the Worldwide LHC Computing Grid (WLCG) infrastructure that is used to run data processing applications.</p> <p>The primary use case of distributing software is a particular one, since software often comprises many small files that are frequently opened and read as a whole, and frequent look-ups for files in multiple directories are triggered when search paths are examined.</p> <p>In certain cases, the CernVM-FS has also been used to distribute large data repositories.</p>"},{"location":"cvmfs/what-is-cvmfs/#features","title":"Features","text":""},{"location":"cvmfs/what-is-cvmfs/#features-ondemand","title":"On-demand downloading of files and metadata","text":"<p>The metadata and content of files included in a CernVM-FS repository are automatically downloaded on-demand as files and directories are being accessed, which is akin to streaming services for music, movies, and TV series.</p> <p>This happens fully transparently, as the contents of a repository are exposed by CernVM-FS as if it were a local (read-only) file system. Hence, clients that access a CernVM-FS repository typically do not actually have a local copy of all files included in that repository, but only have a limited set of files and metadata directly available: those which were most recently accessed.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-updating","title":"Automatic updates","text":"<p>CernVM-FS clients automatically pull in updates to the contents of a repository as they are published server-side. This happens in transactions, to ensure that clients observe a consistent state of the repository.</p> <p>Once a CernVM-FS repository is accessible on a client system, no subsequent actions must be taken to keep clients up-to-date other than updating CernVM-FS itself on a regular basis.</p> <p>This significantly limits the maintenance burden, since no action is required on client systems to update the software stack that is provided through a CernVM-FS repository, since the updates are streamed in automatically by CernVM-FS.</p> <p>Only the CernVM-FS client should be updated on a regular basis on client systems.</p> <p>For more elaborate setups that involve proxies or CernVM-FS replica (mirror) servers, additional maintenance is necessary, but again only to update the CernVM-FS components themselves.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-caching","title":"Multi-level caching","text":"<p>CernVM-FS uses a multi-level caching hierarchy to reduce the latency observed when accessing repository contents. Caching is an essential part of CernVM-FS, since the contents of a CernVM-FS repository are downloaded on-demand as they are accessed.</p> <p>The caching mechanism employed by CernVM-FS goes way beyond the standard (in-memory) Linux kernel file system cache, and consists of a local client cache, an optional forward proxy server that acts as an intermediary cache level, and a distributed network of mirror servers that support the CernVM-FS repository being accessed.</p> <p>When a part of the repository is being accessed that is not available yet in the local client cache, CernVM-FS will traverse the multi-level cache hierarchy to obtain the necessary data and update the local client cache with it, so the files being accessed can be served with low latency.</p> <p>Proxy and mirror servers scale horizontally: the CernVM-FS client makes automatic use of multiple deployed service instances for load-balancing and high-availability.</p> <p>We will explore this multi-level caching mechanism in more detail in this tutorial.</p> <p>See here more technical details on CernVM-FS caching.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-deduplication","title":"De-duplication of files","text":"<p>CernVM-FS stores the contents of a file only once, even when it is included multiple times in a particular repository at different paths.</p> <p>This can result in a significant reduction in storage capacity that is required to host a large software stack, especially when identical files are spread out across the repository, as often happens with particular files like example data files across multiple versions of the same software.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-compression","title":"Compression of data","text":"<p>CernVM-FS stores file content compressed on the server, which not only further reduces required storage space but also significantly limits the network bandwidth that is required to download (and serve) the contents of a repository.</p> <p>On the client side, the data is transparently decompressed when the files included in a CernVM-FS repository are presented under <code>/cvmfs</code> as a normal (read-only) file system.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-data-integrity","title":"Verification of data integrity","text":"<p>The integrity of data provided by a CernVM-FS server is ensured on a client system by verifying a cryptographic hash, which is again a direct result of content-addressable storage mechanism that is used by CernVM-FS. This is an essential security aspect since CernVM-FS uses (possibly untrusted) caches and HTTP connections to distribute the contents of a repository.</p> <p>(next: Technical details of CernVM-FS)</p>"},{"location":"eessi/","title":"EESSI","text":""},{"location":"eessi/#european-environment-for-scientific-software-installations","title":"European Environment for Scientific Software Installations","text":"<ul> <li>What is EESSI?</li> <li>Motivation &amp; Goals</li> <li>Inspiration</li> <li>High-level design</li> <li>Using EESSI</li> </ul>"},{"location":"eessi/high-level-design/","title":"High-level design of EESSI","text":"<p>The design of EESSI is very similar to that of the Compute Canada software stack it is inspired by, and is aligned with the motivation and goals of the project.</p> <p>In the remainder of this section of the tutorial, we will explore the layered structure of the EESSI software stack, and how to use it.</p> <p>In the next section will cover in detail how you can get access to EESSI (and other publicly available CernVM-FS repositories).</p>"},{"location":"eessi/high-level-design/#layered-structure","title":"Layered structure","text":"<p>The EESSI project consists of 3 layers, which are constructed by leveraging various open source software projects.</p> <p> </p>"},{"location":"eessi/high-level-design/#filesystem_layer","title":"Filesystem layer","text":"<p>The filesystem layer is responsible for distributing the EESSI software stack to systems on which is it used.</p> <p>This is done using CernVM-FS, which is a mature open source software project that was created exactly for this purpose: to distribute software installations worldwide reliably and efficiently in a scalable way. As such, it aligns very well with the goals of EESSI.</p> <p>The CernVM-FS repository for EESSI is <code>/cvmfs/software.eessi.io</code>, which is part of the default CernVM-FS configuration since 21 November 2023, so no additional action is required to gain access to it other than installing and configuration the client component of CernVM-FS.</p> <p>More on that in the next section of this tutorial.</p> Note on the EESSI pilot repository (click to expand) <p>There is also a \"pilot\" CernVM-FS repository for EESSI (<code>/cvmfs/pilot.eessi-hpc.org</code>), which was primarily used to gain experience with CernVM-FS in the early years of the EESSI project.</p> <p>Although it is still available currently, we do not recommend using it.</p> <p>Not only will you need to install the CernVM-FS configuration for EESSI to gain access to it, there also are no guarantees that the EESSI pilot repository will remain stable or even available, nor that the software installations it provides are actually functional, since it may be used for experimentation purposes by the EESSI maintainers.</p>"},{"location":"eessi/high-level-design/#compat_layer","title":"Compatibility layer","text":"<p>The compatibility layer of EESSI levels the ground across different (versions of) the Linux operating system (OS) of client systems that use the software installations provided by EESSI.</p> <p>It consists of a limited set of libraries and tools that are installed in a non-standard filesystem location (a \"prefix\"), which were built from source for the supported CPU families using Gentoo Prefix.</p> <p>The installation path of the EESSI compatibility layer corresponds to the <code>compat</code> subdirectory of a specific version of EESSI (like <code>2023.06</code>) in the EESSI CernVM-FS repository, which is specific to a particular type of OS (currently only <code>linux</code>) and CPU family (currently <code>x86_64</code> and <code>aarch64</code>):</p> <pre><code>$ ls /cvmfs/software.eessi.io/versions/2023.06/compat\nlinux\n\n$ ls /cvmfs/software.eessi.io/versions/2023.06/compat/linux\naarch64  x86_64\n\n$ ls /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64\nbin  etc  lib  lib64  opt  reprod  run  sbin  stage1.log  stage2.log  stage3.log  startprefix  tmp  usr  var\n\n$ ls -l /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib64\ntotal 4923\n-rwxr-xr-x 1 cvmfs cvmfs  210528 Nov 15 11:22 ld-linux-x86-64.so.2\n...\n-rwxr-xr-x 1 cvmfs cvmfs 1876824 Nov 15 11:22 libc.so.6\n...\n-rwxr-xr-x 1 cvmfs cvmfs  911600 Nov 15 11:22 libm.so.6\n...\n</code></pre> <p>Libraries included in the compatibility layer can be used on any Linux client system, as long as the CPU family is compatible and taken into account.</p> <pre><code>$ uname -m\nx86_64\n\n$ cat /etc/redhat-release\nRed Hat Enterprise Linux release 8.8 (Ootpa)\n\n$ /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib64/libc.so.6\nGNU C Library (Gentoo 2.37-r7 (patchset 10)) stable release version 2.37.\n...\n</code></pre> <p>By making sure that the software installations included in EESSI only rely on tools and libraries provided by the compatibility layer, and do not (directly) require anything from the client OS, we can ensure that they can be used in a broad variety of Linux systems, regardless of the (version of) Linux distribution being used.</p> <p>Note</p> <p>This is very similar to the OS tools and libraries that are included in container images, except that no container runtime is involved here (typically), only CernVM-FS.</p>"},{"location":"eessi/high-level-design/#software_layer","title":"Software layer","text":"<p>The top layer of EESSI is called the software layer, which contains the actual scientific software applications and their dependencies.</p>"},{"location":"eessi/high-level-design/#easybuild","title":"EasyBuild to install software","text":"<p>Building, managing, and optimising the software installations included in the software layer is layer is done using EasyBuild, a well-established software build and installation framework for managing (scientific) software stacks on High-Performance Computing (HPC) systems.</p>"},{"location":"eessi/high-level-design/#lmod","title":"Lmod as user interface","text":"<p>Next to installing the software itself, EasyBuild also automatically generates environment module files. These files, which are essentially small Lua scripts, are consumed via Lmod, a modern implementation of the concept of environment modules which provides a user-friendly interface to end users of EESSI.</p>"},{"location":"eessi/high-level-design/#cpu_detection","title":"CPU detection via <code>archspec</code> or <code>archdetect</code>","text":"<p>The initialisation script that is included in the EESSI repository automatically detects the CPU family and microarchitecture of a client system by leveraging either <code>archspec</code>, a small Python library, or <code>archdetect</code>, a minimal pure bash implementation of the same concept.</p> <p>Based on the features of the detected CPU microarchitecture, the EESSI initialisation script will automatically select the best suited subdirectory of the software layer that contains software installations that are optimised for that particular type of CPU, and update the session environment to start using it.</p>"},{"location":"eessi/high-level-design/#software_layer_structure","title":"Structure of the software layer","text":"<p>For now, we just briefly show the structure of <code>software</code> subdirectory that contains the software layer of a particular version of EESSI below.</p> <p>The <code>software</code> subdirectory is located at the same level as the <code>compat</code> directory for a particular version of EESSI, along with the <code>init</code> subdirectory that provides initialisation scripts:</p> <pre><code>$ cd /cvmfs/software.eessi.io/versions/2023.06\n$ ls\ncompat  init  software\n</code></pre> <p>In the <code>software</code> subdirectory, a subtree of directories is located that contains software installations that are specific to a particular OS family (only <code>linux</code> currently) and a specific CPU microarchitecture (with <code>generic</code> as a fallback):</p> <pre><code>$ ls software\nlinux\n\n$ ls software/linux\naarch64  x86_64\n\n$ ls software/linux/aarch64\ngeneric  neoverse_n1  neoverse_v1\n\n$ ls software/linux/x86_64\namd  generic  intel\n\n$ ls software/linux/x86_64/amd\nzen2  zen3\n\n$ ls software/linux/x86_64/intel\nhaswell  skylake_avx512\n</code></pre> <p>Each subdirectory that is specific to a particular CPU microarchitecure provides the actual optimised software installations (in <code>software</code>) and environment module files (in <code>modules/all</code>).</p> <p>Here we explore the path that is specific to AMD Milan CPUs, which have the Zen3 microarchitecture, focusing on the installations of OpenBLAS:</p> <pre><code>$ ls software/linux/x86_64/amd/zen3\nmodules  software\n\n$ ls software/linux/x86_64/amd/zen3/software\n\n... (long list of directories of software names omitted) ...\n\n$ ls software/linux/x86_64/amd/zen3/software/OpenBLAS/\n0.3.21-GCC-12.2.0  0.3.23-GCC-12.3.0\n\n$ ls software/linux/x86_64/amd/zen3/software/OpenBLAS/0.3.23-GCC-12.3.0/\nbin  easybuild  include  lib  lib64\n\n$ ls software/linux/x86_64/amd/zen3/modules/all\n\n... (long list of directories of software names omitted) ...\n\n$ ls software/linux/x86_64/amd/zen3/modules/all/OpenBLAS\n0.3.21-GCC-12.2.0.lua  0.3.23-GCC-12.3.0.lua\n</code></pre> <p>Each of the other subdirectories for specific CPU microarchitectures will have the exact same structure, and provide the same software installations and accompanying environment module files to access them with Lmod.</p> <p>A key aspect here is that binaries and libraries that make part of the software installations included in the EESSI software layer only rely on libraries provided by the compatibility layer and/or other software installations in the EESSI software layer.</p> <p>See for example libraries to which the OpenBLAS library links:</p> <pre><code>$ ldd software/linux/x86_64/amd/zen3/software/OpenBLAS/0.3.23-GCC-12.3.0/lib/libopenblas.so\n    linux-vdso.so.1 (0x00007ffd4373d000)\n    libm.so.6 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libm.so.6 (0x000014d0884c8000)\n    libgfortran.so.5 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgfortran.so.5 (0x000014d087115000)\n    libgomp.so.1 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgomp.so.1 (0x000014d088480000)\n    libc.so.6 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libc.so.6 (0x000014d086f43000)\n    /lib64/ld-linux-x86-64.so.2 (0x000014d08837e000)\n    libpthread.so.0 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libpthread.so.0 (0x000014d088479000)\n    libdl.so.2 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libdl.so.2 (0x000014d088474000)\n    libquadmath.so.0 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libquadmath.so.0 (0x000014d08842d000)\n    libgcc_s.so.1 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgcc_s.so.1 (0x000014d08840d000)\n</code></pre> Note on <code>/lib64/ld-linux-x86-64.so.2</code> (click to expand) <p>The <code>/lib64/ld-linux-x86-64.so.2</code> path, which corresponds to the dynamic linker/loader of the Linux client OS, that is shown in the output of <code>ldd</code> above is a bit misleading.</p> <p>It only pops up because we are running the <code>ldd</code> command provided by the client OS, which typically resides at <code>/usr/bin/ldd</code>.</p> <p>When actually running software provided by the EESSI software layer, the loader provided by the EESSI compatibility layer is used to launch binaries.</p> <p>We will explore the EESSI software layer a bit more in the next subsection of this tutorial, when we demonstrate how to use the software installations provided in the EESSI software layer.</p> <p>(next: Using EESSI)</p>"},{"location":"eessi/inspiration/","title":"Inspiration for EESSI","text":"<p>The EESSI concept is heavily inspired by software stack provided by the Digital Research Alliance of Canada (a.k.a. The Alliance, formerly known as Compute Canada), which is a shared software stack used on all national host sites for Advanced Research Computing in Canada that is distributed across Canada (and beyond) using CernVM-FS; see also here.</p> <p>EESSI is significantly more ambitious in its goals however, in various ways.</p> <p>It intends to support a broader range of system architectures than what is currently supported by the Compute Canada software stack, like Arm 64-bit microprocessors, accelerators beyond NVIDIA GPUs, etc.</p> <p>In addition, EESSI is set up to be a community project, by setting up services and infrastructure to automate the software build and installation process as much as possible, providing extensive documentation and support to end users, user support teams, and system administrators who want to employ EESSI, and allowing contributors to propose additions to the software stack.</p> <p>The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\".</p> <p>It has also been presented at the 5th EasyBuild User Meeting, see slides and talk recording.</p> <p>More information on the Compute Canada software stack is available in their documentation, and in their overview of available software.</p> <p>(next: High-level Overview of EESSI)</p>"},{"location":"eessi/motivation-goals/","title":"Motivation &amp; Goals of EESSI","text":""},{"location":"eessi/motivation-goals/#motivation","title":"Motivation","text":"<p>EESSI is motivated by the observation that the landscape of computational science is changing in various ways, including:</p> <ul> <li>Increasing diversity in system architectures: additional families of general-purpose   microprocessors including Arm 64-bit (<code>aarch64</code>) and   RISC-V on top of the well-established Intel and AMD processors (both <code>x86_64</code>),   and different types of GPUS (NVIDIA, AMD, Intel);</li> <li>Rapid expansion of computational science beyond traditional domains like physics and computational chemistry,   including bioinformatis, Machine Learning (ML) and Artificial Intelligence (AI), etc.,   which leads to a significant growth of the software stack that is used for running scientific workloads;</li> <li>Emergence of commercial cloud infrastructure (Amazon EC2,   Microsoft Azure, ...)   that has competitive advantages over on-premise infrastructure for computational workloads, such as near-instant   availability, increased flexibility, a broader variety of hardware platforms, and faster access to   new generations of microprocessors;</li> <li>Limited manpower that is available in the HPC user support teams that are responsible for helping   scientists with running the software they require on high-end (and complex) infrastructure like supercomputers   (and beyond);</li> </ul> <p>Collectively, these indicate that there is a strong need for more collaboration on building and installing scientific software to avoid duplicate work across computational scientists and HPC user support teams.</p>"},{"location":"eessi/motivation-goals/#goals","title":"Goals","text":"<p>The main goal of EESSI is to provide a collection of scientific software installations that work across a wide range of different platforms, including HPC clusters, cloud infrastructure, and personal workstations and laptops, without making compromises on the performance of that software.</p> <p>While initially the focus of EESSI is to support Linux systems with established system architectures like AMD + Intel CPUs and NVIDIA GPUs, the ambition is to also cover emerging technologies like Arm 64-bit CPUs, other accelerators like the AMD Instinct and Intel Xe, and eventually also RISC-V microprocessors.</p> <p>The software installations included in EESSI are optimized for specific generations of microprocessors by targeting a variety of instruction set architectures (ISAs), like for example Intel and AMD processors supporting the AVX2 or AVX-512 instructions, and Arm processors that support SVE instructions.</p> <p>(next: Inspiration for EESSI)</p>"},{"location":"eessi/support/","title":"Getting support for EESSI","text":"<p>Thanks to the funding provided by the MultiXscale EuroHPC JU Centre-of-Excellence, a dedicated support team is available to provide help on accessing or using EESSI.</p> <p>If you have any questions, or if you are experiencing problems, do not hesitate to reach out by either opening an issue in the EESSI support portal, or sending an email to <code>support@eessi.io</code>.</p> <p>For more information, see the support section of the EESSI documentation.</p> <p>(next: Setting up a CernVM-FS client system)</p>"},{"location":"eessi/using-eessi/","title":"Using EESSI","text":"<p>Using the software installations provided by the EESSI CernVM-FS repository <code>software.eessi.io</code> is fairly straightforward.</p> <p>Let's break it down step by step.</p>"},{"location":"eessi/using-eessi/#0-is-eessi-available","title":"0) Is EESSI available?","text":"<p>First, check whether the EESSI CernVM-FS repository is available on your system.</p> <p>Try checking the contents of the <code>/cvmfs/software.eessi.io</code> directory with the <code>ls</code> command:</p> <pre><code>$ ls /cvmfs/software.eessi.io\nhost_injections  README  versions\n</code></pre> <p>If you see an error message like \"<code>No such file or directory</code>\", then either the CernVM-FS client is not installed on your system, or the configuration for the EESSI repository is not available. In that case, you may want to revisit the Accessing a CernVM-FS repository section, or go through the Troubleshooting section.</p> Don't be fooled by <code>autofs</code> (click to expand) <p>The <code>/cvmfs</code> directory may seem empty at first, because CernVM-FS repositories are automatically mounted as they are accessed via <code>autofs</code>.</p> <p>So rather than just using \"<code>ls /cvmfs/</code>\" to check which CernVM-FS repositories are available on your system, you should try to directly access a specific repository as shown above for EESSI with <code>ls /cvmfs/software.eessi.io</code> .</p> <p>For more information on various aspects of mounting of CernVM-FS repositories, see the CernVM-FS documentation.</p>"},{"location":"eessi/using-eessi/#1-initialise-shell-environment","title":"1) Initialise shell environment","text":"<p>If the EESSI repository is available, you can proceed to preparing your shell environment for using a particular version of EESSI by sourcing the provided initialisation script by running the <code>source</code> command:</p> <pre><code>$ source /cvmfs/software.eessi.io/versions/2023.06/init/bash\nFound EESSI repo @ /cvmfs/software.eessi.io/versions/2023.06!\narchdetect says x86_64/amd/zen2\nUsing x86_64/amd/zen2 as software subdirectory.\nUsing /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI (2023.06), have fun!\n</code></pre> Details on changes made to the shell environment (click to expand) <p>The initialisation script is a simple bash script that changes a couple of environment variables:</p> <ul> <li>A set of <code>$EESSI_*</code> environment variables is defined;</li> <li>The <code>$PS1</code> environment variable that specifies the shell prompt   is updated to indicate that your shell session has been initialised for EESSI;</li> <li>The location of the tools provided by the EESSI compatibility layer are prepended to the <code>$PATH</code> environment variable;</li> <li>Lmod, which is included in the EESSI compatibility layer, is initialised to ensure that the <code>module</code> command is defined,   and that the Lmod spider cache that is included in the EESSI software layer is picked up;</li> <li>The location to the software installations that are optimised for the CPU microarchitecture of the client system   is prepended to the <code>$MODULEPATH</code> environment variable by running a \"<code>module use</code>\" command.</li> </ul> <p>Note how the CPU microarchitecture is being auto-detected, which determines which path that points to a set of environment module files is used to update <code>$MODULEPATH</code>.</p> <p>This ensures that the modules that will be loaded provide access to software installations from the EESSI software layer that are optimised for the system you are using EESSI on.</p>"},{"location":"eessi/using-eessi/#2-load-modules","title":"2) Load module(s)","text":"<p>After initialising your shell environment for using EESSI, you can start exploring the EESSI software layer using the <code>module</code> command.</p> <p>Using <code>module avail</code> (or <code>ml av</code>), you can check which software is available. Without extra arguments, <code>module avail</code> will produce an overview of all available software. By passing an extra argument you can filter the results and search for specific software:</p> <pre><code>$ module avail tensorflow\n\n----- /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all -----\n\n    TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>To start using software you should load the corresponding environment module files using <code>module load</code> (or <code>ml</code>). For example:</p> <pre><code>$ module load TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>A <code>module load</code> command usually does not produce any output, but it updates your shell environment to make the software ready to use.</p> <p>For more information on the <code>module</code> command, see the User Guide for Lmod.</p>"},{"location":"eessi/using-eessi/#3-use-software","title":"3) Use software","text":"<p>After loading a module, you should be able to use the corresponding software.</p> <p>For example, after loading the <code>TensorFlow/2.13.0-foss-2023a</code> module, you can start a Python session and play with the <code>tensorflow</code> Python package:</p> <pre><code>$ python\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.13.0'\n</code></pre> <p>Keep in mind that you are using a Python installation provided by the EESSI software layer here, not the Python version that may be provided by your client OS:</p> <pre><code>$ command -v python\n/cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/software/Python/3.11.3-GCCcore-12.3.0/bin/python\n</code></pre> Initial start-up delay (click to expand) <p>You may notice a bit of \"lag\" initially when starting to use software provided by the EESSI software layer.</p> <p>This is expected, since CernVM-FS may need to first download the files that are required to run the software you are using; see also On-demand downloading of files and metadata.</p> <p>You should not observe any significant start-up delays anymore when running the same software shortly after, since then CernVM-FS will be able to serve the necessary files from the local client cache; see also Multi-level caching.</p> <p>(next: Getting support for EESSI)</p>"},{"location":"eessi/what-is-eessi/","title":"What is EESSI?","text":"<p>The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in the HPC (High Performance Computing) community.</p> <p>EESSI provides a common stack of optimized scientific software installations that work on any Linux distribution, and currently supports both <code>x86_64</code> (AMD/Intel) and <code>aarch64</code> (Arm 64-bit) systems, which is distributed via CernVM-FS.</p> <p>(next: Motivation &amp; Goals of EESSI)</p>"}]}